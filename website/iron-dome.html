<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>CS225A Iron Dome Project</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="Daniel,Hayk,Wanxi" >

	<link href="css/bootstrap.min.css" rel="stylesheet">
	<link href="css/style.css" rel="stylesheet">

	<script type="text/javascript" src="js/jquery.min.js"></script>
	<script type="text/javascript" src="js/bootstrap.min.js"></script>
	<script type="text/javascript" src="js/scripts.js"></script>
  <script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
</head>

<body>
<div class="container">
	<div class="row clearfix">

		<div class="col-md-12 column">

			<!-- CS225A WEBSITE HEADER START -->
			<nav class="navbar navbar-default" role="navigation">
				<div class="navbar-header">
					 <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
					 <span class="sr-only">Toggle navigation</span><span class="icon-bar"></span>
					 <span class="icon-bar"></span>
					 <span class="icon-bar"></span>
					 </button>
					 <a class="navbar-brand" >CS225A</a>
				</div>

				<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
					<ul class="nav navbar-nav">
						<li>
							<a href="http://cs.stanford.edu/groups/manips/teaching/cs225a/">Home</a>
						</li>
						<li>
							<a href="http://cs.stanford.edu/groups/manips/teaching/cs225a/projects.html">Projects</a>
						</li>
						<li>
							<a href="http://web.stanford.edu/~smenon/scl.html" target="_blank">Control & Simulation</a>
						</li>
						</li>
					</ul>
					<ul class="nav navbar-nav navbar-right">
						<li>
							<a href="http://piazza.com/stanford/fall2014/cs225a" target="_blank">Piazza</a>
						</li>
						<li>
							<a href="http://cs.stanford.edu/groups/manips/" target="_blank">Stanford Robotics</a>
						</li>
					</ul>
				</div>

			</nav>
			<!-- CS225A WEBSITE HEADER END -->

			<!-- PERSONALIZED HEADER PICTURES -->
      <div class="row">
  			<div class="carousel slide col-md-offset-3 col-md-6" id="carousel-25687">
  				<ol class="carousel-indicators">
  					<li class="active" data-slide-to="0" data-target="#carousel-25687">
  					</li>
  					<li data-slide-to="1" data-target="#carousel-25687">
  					</li>
  				</ol>
  				<div class="carousel-inner">
            <div class="item video active">
              <iframe width="560" height="315" src="http://www.youtube.com/embed/1mfjbcGPI2w"
              frameborder="0" allowfullscreen></iframe>
            </div>
            <div class="item video">
              <iframe width="560" height="315" src="http://www.youtube.com/embed/rPIiMUTwbWA"
              frameborder="0" allowfullscreen></iframe>
            </div>
            <div class="item video">
              <iframe width="560" height="315" src="http://www.youtube.com/embed/A24_O-sdZww"
              frameborder="0" allowfullscreen></iframe>
            </div>
  				</div> <a class="left carousel-control" href="#carousel-25687" data-slide="prev"><span class="glyphicon glyphicon-chevron-left"></span></a> <a class="right carousel-control" href="#carousel-25687" data-slide="next"><span class="glyphicon glyphicon-chevron-right"></span></a>
  			</div>
      </div>

    <div class="row">

			<!-- ABSTRACT -->
			<div class="col-md-12 clearfix">
				<h2>Iron Dome, Fall 2014</h2>
        <p>
          The Iron Dome is a projectile defense system using the Kuka LBR iiwa robot.
          The system dynamically identifies incoming projectiles headed toward a target,
          estimates their trajectories, and intercepts them in a fraction of a second.
        </p>
				<p>
          Our vision system uses an off-the-shelf Kinect V2 to  detect and label incoming
          projectiles. The measurements are sent through a Kalman filter and trajectory
          predictor to determine the intersecting time, position and orientation with the
          robot defensive zone. The robot is then commanded to move to the desired configuration
          in advance of the projectile(s), preventing them from hitting the target. To improve
          robustness, stability, and reliablity of the control of the robot, we run our task
          space position and orientation controller in software, adding a tangent potential
          field to push each joint away from reaching joint limit, and clamping position and
          orientation errors to avoid large motions. We then feed joint angles directly from
          our simulated result to the physical robot, which runs a very fast joint position
          controller. The result is a system which can dynamically respond to thrown projectiles
          from a distance of as little as three meters.
				</p>
        <p>
          Code for this project is publicly available on the
          <a href="https://github.com/hmartiro/iron-dome/">iron-dome</a> Github repository.
        </p>
			</div>

			<!-- TEAM MEMBERS -->
			<div class="col-md-12 clearfix">
				<h3> Members </h3>
				<div class="row clearfix">
					<div class="col-md-3 column">
						<h4> Hayk Martirosyan </h4>
						<p>
							<a href="http://haykmartirosyan.com/" target="_blank">
								<img alt="" src="pictures/hayk.jpg" height="150" width="150">
							</a>
						</p>
					</div>
					<div class="col-md-3 column">
						<h4> Daniel O'Shea </h4>
						<p>
							<a href="https://www.linkedin.com/in/danielmoshea" target="_blank">
								<img alt="" src="pictures/daniel.jpg" height="150" width="150">
							</a>
						</p>
					</div>
					<div class="col-md-3 column">
						<h4> Wanxi Liu </h4>
						<p>
							<a href="https://www.linkedin.com/in/wancy" target="_blank">
								<img alt="" src="pictures/wanxi.jpg" height="150" width="150">
							</a>
						</p>
					</div>
				</div>
			</div>

			<div class="col-md-12 clearfix">

				<h3>High-level System</h3>
				<p>
					The physical robot is commanded using direct open-loop joint-space control, where the joint positions commanded
					are the joint positions of the robot's counterpart in our real-time simulation. The simulation
					and command are both done at 10kHz. The rest of the robot's operation can be explained by a
          description of the simulation. Our simulation was an essential tool for this project because it's modularity
          allowed us to easily command the simulated robot or the real robot, interchange simulated projectile data for real data
          from the vision system, and rapidly debug and test different controllers.
				</p>

        <h4>State Machine</h4>
        <p>
          When the robot is actively searching for projectiles, it is in a rest state and moves to a position we have chosen
          to allow it to quickly reach the area a projectile is likely to enter its workspace. At this point
          the vision system is searching for valid targets.
        </p>
        <p>
          When an incoming projectile is found, we use a Kalman filter to predict the trajectory. Once the trajectory
          converges, the next state depends on if the predicted trajectory intersects the interception boundary (a sphere
          section). If it does not intersect, the robot returns to searching for a projectile.
          If more than one projectile is detected at once, this trajectory prediction runs simultaneously, but the robot
          evaluates the projectiles as targets in the order that the predicted trajectories converge.
        </p>
        <p>
          Once a projectile has been targeted, the robot tries to move to the position at which the projectile will intersect
          its workspace. The orientation is calculated to be opposite the velocity vector of the projectile at
          interception. It waits in this configuration until hitting the ball, then returns to the rest state.
        </p>

				<h4>Trajectory Prediction</h4>
				<p>
					The robot intercepts projectiles on a floating window in front of it, which is
					a bounded section of a sphere. For each projectile the robot can see, the trajectory is
					predicted based on the observations from the Kinect depth image, which are fed into a Kalman filter
					to deal with the noise and any problems with detection of the ball. The intersection point is found
					by solving the following system of equations:
				</p>
					\begin{align*}
					x &= x_0+v_x*t+\frac{1}{2}a_x^2\\
					y &= y_0+v_y*t+\frac{1}{2}a_y^2\\
					z &= z_0+v_z*t+\frac{1}{2}a_z^2\\
					R^2 &= (x_s-x)^2+(y_s-y)^2+(z_s-z)^2
					\end{align*}
				<p>
					Where \([x,y,z]\) is the position of the ball with respect to the robot base frame.
					The acceleration in z is assumed to be \(-9.81 \frac{m}{s}\) while in x and y it
					is assumed to be zero. The radius \(R\) and position of the sphere are chosen such that the window
					in front of the robot is within the usable workspace of the robot. We used a polynomial solver
					on the fourth order equation in \(t\) to find the time and location
					that the predicted trajectory would intercept the sphere. If one or more real roots exist, then
					there will be an intersection at the time of the lowest real root.
				</p>
      </div>

      <div class="col-md-12 clearfix">
				<h3>Control Algorithm</h3>
				<p>
			    Below are the equations to calculate the commanded generalized
			    forces, \(\tau\), of our robot. Subscripts P and R stand for
			    position and rotation, respectively, and subscripts C and D
			    stand for current and desired.
			    <br /><br />
        </p>
        <p>
          \begin{align}
          \tau_{total} &= \tau +G(q)\\
          \tau &= J^{T}(\Lambda F)\\
          \Lambda &= (JM^{-1}J^{T})^{-1}\\F_{P} &= -k_{pP}*dx - k_{vP}*\dot{x}\\
          F_{R} &= -k_{pR}*d\phi - k_{vR}*\omega\\
          dx &= x_{C} - x_{D}\\
          d\phi &= -\frac{1}{2}(s_{1C}\times s_{1D}+s_{2C}\times s_{2D}+s_{3C}\times s_{3D})\\
          \omega &= J_{R}*\dot{q}\\
          \dot{x} &= J_{P}*\dot{q}
          \end{align}

          \[ R_{CO} =
          \begin{bmatrix}
          \vert & \vert & \vert\\
          s_{1C} & s_{2C} & s_{3C}\\
          \vert & \vert & \vert\\
          \end{bmatrix}\\
          R_{DO} =
          \begin{bmatrix}
          \vert & \vert & \vert\\
          s_{1D} & s_{2D} & s_{3D}\\
          \vert & \vert & \vert\\
          \end{bmatrix}
          \]
        </p>
				<p>
					Our control implementation for simulation is based on giving the robot a desired position
					and orientation in order to meet the projectile hurling rapidly at it, and using the known dynamics
					of the robot to calculate the necessary joint torques to reach the commanded configuration (1).
					The forces for the translational and rotational parts of the controller
					are calculated separately and then combined, with the position and rotation parts denoted
					by the subscripts P and R.
				</p><p>
					We use the pseudo-inverse method to find the task space mass matrix \(\Lambda\) (3)
					from the joint space matrix. We find the task space torques using PD control (4)(5),
					using the error in position (6) for the cartesian force and error in orientation (7)
					for the moments \(F_{R}\).
				</p><p>
					The error in orientation (7) is found by taking the cross product of each column vector in the
					current rotation matrix \(R_{CO}\) by the column vectors in the matrix
					representing the desired rotation \(R_{DO}\). This gives a torque axis and
					magnitude which the proportional rotation gain \(k_{pR}\) is applied to.
        </p><p>
          Additionally, in practice we clamped the position and orientation error vectors to a maximum magnitude,
          to avoid commanding desired positions that are too far away.
        </p>

        <h3>Simulated Projectile Generation</h3><br/>
        <p>
          To test our control algorithms and state machine separately from the vision system,
          we created a separate program that generates fake projectile measurement data based
          on statistical distributions. This program reports projectile IDs, timestamps, and
          noisy position measurements with an identical interface and with the same frequency
          as the real vision system.
        </p>
        <p>
          The projectiles are generated based on a Poisson distribution that models random
          events. The initial positions and velocities of the projectiles are based on a
          multivariate Gaussian distribution. Observations are reported by calculating
          the result of the projectile motion equations with the current time, and then
          adding Gaussian noise along each axis. Adjustable parameters include the average
          throw time, the mean and variance of the initial position and velocities, the
          average throw angle, and the measurement noise standard deviation.
        </p>
        <p>
          This program helped us understand the requirements of our system, as we could run trials
          fully in simulation and see what kind of response times and trajectories we needed to
          successfully intercept projectiles. It also helped us tune the Kalman filter that allows
          us to predict the future position of projectiles, which is absolutely critical. The plot
          below shows example trajectories of simulated projectiles.
        </p>

        <img alt="" src="./pictures/projectile_simulation_plot.png" height="400px">

        <h3>Projectile Detection</h3><br />
        <p>
          The vision task of our project is to track multiple projectiles
          and obtain their world coordinates (x, y, z) along with a timestamp.
          The result is used for predicting the trajectories so that we can command
          the robot to a desired position and orientation in advance of the projectile.
          With this in mind, our vision system must meet the requirements of high speed and accuracy.
        </p>
        <p>
          We are using the Kinect for Windows V2 to track incoming projectiles. The depth sensor
          of the Kinect has a frame rate of 30Hz and improved resolution (compared to Kinect V1) of
          512 x 424, as well as a much lower latency. We run a simple blob detector on the depth image
          to segment projectiles from the background. As introduced on the
          <a href="http://docs.opencv.org/modules/features2d/doc/common_interfaces_of_feature_detectors.html#simpleblobdetector">
          OpenCV documentation</a>, a simple blob detector extracts
          blobs by running multiple binary thresholding on the image, finding connected components(contours)
          on each binary images, and grouping close centers of the contours from different binary images.
          As our projectile is a ball with certain size, we also filter by circularity and area.
        </p>

        <img alt= "Depth image with ball coordinates and circle overlay" src="./pictures/screenshot-detect.png" height="300"><br />

        <p>
          After extracting blobs from the image, we further analyze the result to filter out noise based on a radius/depth ratio
          of detected blobs.
          We record the ball radius in pixels versus depth and fit the data into a 3rd order polynomial equation (shown in figure
          below). In our program, we calculate the “desired” pixel radius of each blob using the depth of its center, and filter
          out those with a radius that has more than 10% error from the desired value.
        </p>

        <img alt= "" src="./pictures/r-vs-d.png" height="250">

        <p>
          For better handling of multiple projectiles, we implemented several ID assignment strategies to assign each
          projectile a unique ID. Basically we assume a new projectile is coming if we haven’t detected anything in
          the past second (or less than one second). And we decide whether a projectile is the same one as detected before
          by looking at its centroid distance and the time stamps. For this purpose, detected balls are stored in a first
          come first out buffer container each for 0.3 seconds (approximately 10 frames). A decision flowchart is shown below.
        </p>

        <img alt= "" src="./pictures/flowchart.jpeg" height="300">

        <h4> Frame Matching </h4>
        <p>
          A large challenge was calibrating the frame between the Kinect, which was positioned in front and to the side
          of the Kuka, with the robot base frame. We created a four point calibration method that allowed us to generate an
          accurate tranformation matrix from the kinect frame to the robot base frame. These points can't be coplanar, and we
          found it necessary to use points that were in diverse locations in the camera frame to decrease the error in the
          matrix. One benefit of this approach is that we don't need to fix the Kinect's position and orientation, and
          the orientation and position are found using the Kinect's own sensor. The transformation was applied to
          projectile observations to put them in the robot base frame.
        </p>
			</div>

			<div class="col-md-12 clearfix">
				<h3>References</h3>

				O. Khatib,
				"A Unified Approach for Motion and Force Control of Robot Manipulators:	The Operational Space Formulation"
				<i>, IEEE Journal of Robotics and Automation</i>, RA-3, 1, 43--53, February, 1987.
				<br /><br />
			</div>

    </div>
		</div>
	</div>

	<!-- FOOTER -->
	<div class="hrline">
    <hr />
  </div>
  <br />

	<div class="row clearfix">
		<div class="col-md-3 column">
		</div>
		<div class="col-md-6 column">
				<div class = "text-center">
					<p>
						&nbsp;&nbsp;&nbsp; &#169; Stanford University. <br />
						&nbsp;&nbsp;&nbsp; Last updated on December 11th, 2014
					</p>
				</div>
		</div>
		<div class="col-md-3 column">
		</div>
	</div>
	<br />

</div> <!-- end container -->
</body>
</html>
